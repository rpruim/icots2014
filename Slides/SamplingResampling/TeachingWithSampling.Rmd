---
title: "Teaching with Sampling"
author: "Danny Kaplan"
date: "ICOTS Project MOSAIC Workshop; July 12-13, 2014"
output:
  html_document:
    fig_height: 4
    fig_width: 5
---

```{r include=FALSE}
require(mosaic)
require(dplyr)
require(knitr)
opts_chunk$set( tidy=FALSE, size='small',comment=NA)
```

## The Influence of Technology

Much of what we do is the result of an adaptation to specific technology.

### Example: Computational Instructions

**Technology**: Printing, chalk, pencil, paper

With this physical technology, techniques for teaching were developed.

* Algebraic notation.
* Use of simple quantities: e.g. mean, proportion, standard deviation, ...

These established approaches have merits and demerits:

* **plus** We learned things this way, so we're good at it.
* **plus** They are accepted and considered proper.
* **minus** Students are (broadly speaking) not good at reading or interpreting algebra.
* **minus** Quantities such as the mean are not very engaging.
* **minus** The approach doesn't scale to the needs of most of today's users of statistics.
    * Data are too big to use the algebraic formulas.
    * The algebra requires a lot of background to move beyond the very simple.
        * Multiple regression & linear algebra.
    * Many important statistical approaches don't have algebraic representations.  (Example: What's the formula for fitting a logistic model?)
* **minus** The algebra is a distant and abstract representation of the underlying logic of statistics.
    
**A New Technology**: Software 
    Packaging up operations so that they can be performed knowing only the interface.
    
Examples:
```{r}
xyplot( height ~ father, data=Galton )
```

```{r}
t.test( height ~ sex, data=Galton)
```


## A Challenge and an Opportunity

We can easily provide students with just about any possible statistical operation.

* What operations will help students learn statistical thinking?
* What new ways of interacting with students become possible?

## Fundamental Statistical Operations

Nominate candidates, please

* getting a p-value
* regression in two variables
* getting a sampling distribution
* getting a CI
* identifying problems/cleaning data
* interpreting CI


## My Candidates

* Sample
* Describing Relationships
* Randomization
* Repeating stuff

## TenMileRace

Give this command:
```
help(TenMileRace)
```

Focus on these three variables: `net`, `age`, & `sex`

Take two minutes and tell me what you see.

### What the ICOTS Participants Saw and Concluded ...

A positive relationship between `net` and `age`:

```{r}
xyplot( net ~ age | sex, data=TenMileRace, alpha=.1)
```

```{r}
median( net ~ sex, data=TenMileRace)
```

```{r}
tally(~ sex, data=TenMileRace, format="proportion")
```

### All Together Now ...

Let's have everyone do this same analysis on the same data.  

* Do different people get different results? 
    Explain why or why not.

### A Different Use for These Data

Imagine that you are studying runners' performance.  You need to collect some data.

You can't practically study every runner, so you will take a sample.

## The Most Important Template

<style type="text/css">
span.boxed {
  border:5px solid gray;
  border-radius:10px;
  padding: 5px;
}
span.invboxed {
  border:5px solid gray;
  padding: 5px;
  border-radius:10px;
  color: white;
}
table, td, th
{
border:0px;
}
</style>

&nbsp;
<center>
<h2><strong><span class="boxed">goal</span> ( <span class="boxed">&nbsp;y&nbsp;</span> ~ <span class="boxed">&nbsp;x&nbsp;</span> , data = <span class="boxed">mydata</span> )</strong> 
</h2></center>
 
&nbsp;

We've used this template to do calculations on data.  But what are the data?

One way to think of it ...

&nbsp;
<center>
<h2><strong><span class="boxed">goal</span> ( <span class="boxed">&nbsp;y&nbsp;</span> ~ <span class="boxed">&nbsp;x&nbsp;</span> , data = <span class="boxed">sample(thepopulation)</span> )</strong> 
</h2></center>
 
&nbsp;

## Sampling as an Operation

The mosaic `sample()` function takes a [simple random sample](http://en.wikipedia.org/wiki/Simple_random_sample). 

Example:
```{r}
sample(Galton, size=3)
```

You try it.  Did you get the same result?

## Sampling Produces Variation

Let's look at the proportion of runners of each sex in a sample of size $n=100$.

```{r}
tally( ~ sex, format = 'proportion', 
       data = sample(TenMileRace, size = 100) )
```

Did you get the same result?  Why or why not?

Let's look at your results.

* Ask a couple of students
* Ask others who have much larger or smaller results.
* Beep when I'm pointing near your answer 

![a scale from zero to one](https://dl.dropboxusercontent.com/u/5098197/ICOTS/zero-to-one.png)

## How about $n=400$

```{r}
tally( ~ sex, format = 'proportion', 
       data = sample(TenMileRace, size = 400) )
```

* Beep when I'm pointing near your answer 

![a scale from zero to one](https://dl.dropboxusercontent.com/u/5098197/ICOTS/zero-to-one.png)

## Now, $n=1600$

```{r}
tally( ~ sex, format = 'proportion', 
       data = sample(TenMileRace, size = 1600) )
```

* Beep when I'm pointing near your answer 

![a scale from zero to one](https://dl.dropboxusercontent.com/u/5098197/ICOTS/zero-to-one.png)

## What's the Pattern?

What is the pattern in the spread of answers?

## If you wanted to do it all yourself

```{r}
Trials <- do(1000)* tally( ~ sex, format = 'proportion', 
       data = sample(TenMileRace, size = 400) )
freqpolygon( ~ F, data = Trials )
```

Tell me how to compute the standard error.

Do this calculation for sample sizes $n=$100, 400, 1600 and tell me the standard error:

  n | SE
----|----
100 | ?
100 | ?
400 | ?
400 | ?
1600|  ?
1600|

## Schematic of the Sampling Distribution

![sampling distribution](https://dl.dropboxusercontent.com/u/5098197/ICOTS/sampling-distribution.png)

## Back toward the Real World

We don't often have the chance to get multiple samples from our population, like the 1000 samples taken above.

* Samples are expensive, time consuming, and potentially destructive.
* If we could take many samples, we would be well to combine them into one larger sample.

## Resampling Distribution

![resampling distribution](https://dl.dropboxusercontent.com/u/5098197/ICOTS/resampling-distribution.png)

## The resample() function

As an example, start with a very small data set:

```{r}
GaltonSmall <- sample(Galton, size = 5)
GaltonSmall
```

What's going on with `resample()`?

```{r}
resample( GaltonSmall )
```

You do it several times.

## Contest

Can anyone get a resample with two or fewer of the original cases?

How about the same case repeated five times?

## The Re-Sampling Distribution of the Mean (or whatever)

```{r}
Trials <- do(1000) * mean( ~ height, data = resample(Galton) )
# Now find the standard error
```

### Example: Do Husbands and Wives have Related Heights

Use the correlation coefficient: `cor(father ~ mother, data=)`

What's your answer?  

#### Technical Note: 

The case in `Galton` is a child.  We should look at the relationship between husband and wife based on families.  An instructor might want to give students the commands to do this, without having the student try to construct them.

```{r}
ByFamily <- Galton %>% group_by(family) %>% sample_n(size=1) 
```

Use `ByFamily` to find the correlation and it's resampling distribution.



### Example: Do Sons and Fathers have Related Heights

This is the problem Galton studied:

```{r}
Boys <- subset(Galton, sex=='M')
Trials2 <- do(1000)*cor( father ~ height, data = resample(Boys))
```


## Hypothesis Testing

Use `shuffle()`.


What's a typical p-value when the Null Hypothesis is right?

```{r}
NullPvalues <- do(1000)*t.test( height ~ shuffle(sex), data=Galton )$p.value
```





## The Modeling Template

Let's teach the variety of "tests" using one framework that they all have in common.  So put the zoo of tests aside for a moment --- the t-tests, the p-tests, the tests of r and R^2, chi-squared, etc.  Instead, point out that they are all varieties of animal.

The common framework is the linear model: a response variable broken down by one or more explanatory variables.

* Two-sample t-test
```{r}
mod1 <- lm( height ~ sex, data=Galton )
summary(mod1)
```

* Single proportions
```{r}
mod2 <- lm( sex=='F' ~ 1, data=TenMileRace)
confint(mod2)
summary( mod2 )
```

The answer is exactly the same as you would get from the familiar formula: $\sqrt{\hat{p} (1-\hat{p})/n$:
```{r}
sqrt( .5008*(1-.5008)/nrow(TenMileRace))
```

* Difference of two proportions:
    Here, we'll look at whether the fraction of females in the workforce is the same for unionized and non-union workers (in 1985, per the `CPS85` data in `mosaic`).
    ```{r}
mod3 <- lm( sex=="F" ~ union, data=CPS85)
confint(mod3)
summary(mod3)
```
Yes, women appear in a much smaller proportion among unionized workers (in 1985).

* One sample t test
    Is the height of Galton's subjects different from 65 inches?
    `1` stands for a variable that is the same for every case, as if there were a column of ones in the data frame.
    ```{r}
mod4 <- lm( height-65 ~ 1, data = Galton)
confint( mod4 )
summary(mod4)
```

* Multiple regression
    ```{r}
mod5 <- lm( height ~ father + mother + sex, 
            data = Galton)
confint(mod5)
summary(mod5)
```

* Simple regression
    ```{r}
mod6 <- lm( height ~ father, data=Galton )
summary(mod6)
```


* One-way ANOVA
    Here we look at the wage in different sectors of the economy.
    ```{r}
bwplot( wage ~ sector, data=CPS85 )
mod7 <- lm( wage ~ sector, data=CPS85 )
anova(mod7)
```

* Two-way ANOVA
    ```{r}
mod8 <- lm( wage ~ sex + union, data = CPS85)
anova(mod8)
```

* Two-way ANOVA with interaction
    ```{r}
mod9 <- lm( wage ~ sex + union + sex:union, data = CPS85)
anova(mod9)
```

* Analysis of covariance
    ```{r}
mod10 <- lm( wage ~ age + sex, data=CPS85)
summary(mod10)
```

### Note about proportions ...

`lm()` is a perfectly good way to calculate the differences in proportions among discrete groups.

When using multiple explanatory variables, `lm()` can generate proportions outside the range 0 to 1.  The reason is that the linear model is too "stiff."  Fix that by using logistic regression, which is remarkably similar to `lm()`

```{r}
mod11 <- glm( sex=='F' ~ age + educ + union, data=CPS85)
summary(mod11)
```

Admittedly, the coefficients in logistic regression need to be interpreted in terms of ratios of log likelihoods.  That's a mouthful.  But there are two helpful simplifications:

1. A positive coefficient means "higher proportion."  A negative coefficient means "lower proportion."
2. The proportion itself is easily graphed using the function technique.

```{r}
fun11 <- makeFun(mod11)
plotFun( fun11( age=age, union='Not',educ=12)~age,
         age.lim=range(20,80), ylim=range(.3,.7))
plotFun( fun11( age=age, union='Union',educ=12)~age,
         add=TRUE, col='red' )
```
